# Natural Language Processing

# Importing the dataset
dataset_original=read.delim("Restaurant_Reviews.tsv",quote='',stringsAsFactors = FALSE)
View(dataset_original)

# Cleaning the texts
install.packages("tm")
install.packages("NLTK")
install.packages("SpaCy")
install.packages("OpenNLP")
install.packages("SnowballC")
library(tm)
library(SnowballC)
corpus=VCorpus(VectorSource(dataset_original$Review))   # Creating the Corpus
corpus=tm_map(corpus,content_transformer(tolower))  # 1.Convert all text to lower case
corpus=tm_map(corpus,removeNumbers)  # 2. Remove all numbers
corpus=tm_map(corpus,removePunctuation)  # 3. Remove all punctuations
corpus=tm_map(corpus,removeWords,stopwords())  # 4. Remove all stopwords
corpus=tm_map(corpus,stemDocument)  # 5. Steming 
corpus=tm_map(corpus,stripWhitespace)  # 6. Remove extra white spaces

# Creating the Bag of words model
dtm=DocumentTermMatrix(corpus)     # Converting the bag of words to matrix
dtm=removeSparseTerms(dtm,0.999)   # Remove sparse terms at 99% not used
dtm
dataset=as.data.frame(as.matrix(dtm))  # Converting matrix to data frame
View(dataset)
dataset$Liked=dataset_original$Liked  # Creating the depandend variable named Liked

#Taking our Random Forest Excercise into account
# Encoding the target feature as factor
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# No need of Feature Scaling

# Fitting Decision Tree to the Training set
library(randomForest)
classifier=randomForest(x=training_set[-692],
                        y=training_set$Liked,
                        ntree= 10)

# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-692])
y_pred
# Making the Confusion Matrix
cm = table(test_set[, 692], y_pred)
cm





