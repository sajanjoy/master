########### TEXT MINING #############

library(ggplot2)
library(caret)
library(e1071)
library(quanteda)
library(irlba)
library(randomForest)

## Load the data

spam.raw=read.csv("spam.csv",stringsAsFactors = FALSE)
head(spam.raw)
View(spam.raw)


## Clean up the data and view our handiwork
spam.raw=spam.raw[,1:2]
names(spam.raw)
names(spam.raw)=c("Label","Text")
View(spam.raw)


## Checking the missing data
length(which(!complete.cases(spam.raw)))


## Convert our class label into factor
spam.raw$Label=as.factor(spam.raw$Label)

## First step is always to explore the data
## First lets look at the distribution of the data (ie ham vs spam)
table(spam.raw$Label)
prop.table(table(spam.raw$Label))


## Next get a feel of the text length of the SMS
## Messages by adding a new feature for the length of the each message
spam.raw$TextLength=nchar(spam.raw$Text)
View(spam.raw)
summary(spam.raw$TextLength)
boxplot(spam.raw$TextLength)

## Visualize distribution by ggplot2, adding segmentation for ham/spam
ggplot(spam.raw,aes(x=TextLength,fill=Label))+
  theme_bw()+geom_histogram(binwidth = 5)+
  labs(y="Text Count",x="Length of Text",title="Distribution of Text Length with Class Labels")


## Use caret package for 70%/30% stratified split. Set the random
## Seed for reproductibility
set.seed(32984)
indexes=createDataPartition(spam.raw$Label,times = 1,p=0.7,list = FALSE)
train=spam.raw[indexes,]  
test=spam.raw[-indexes,]  


## Verify Proportion
prop.table(table(train$Label))
prop.table(table(test$Label))


## Tokenize SMS text messages
train.tokens=tokens(train$Text,what="word",remove_nu)
train.tokens=tokens(train$Text,what="word",remove_numbers=TRUE,remove_punct=TRUE,
                    remove_symbols=TRUE,remove_hyphens=TRUE)
train.tokens[337]
train.tokens[[357]]

## Lower case the tokens
train.tokens=tokens_tolower(train.tokens)
train.tokens[[357]]

## Stopwords removal
train.tokens=tokens_select(train.tokens,stopwords(),selection = "remove")
train.tokens[[357]]  

## Performing Stemming of Tokens
train.tokens=tokens_wordstem(train.tokens,language = "english")
train.tokens[[357]]

## Create our first bsg of words
train.tokens.dfm=dfm(train.tokens,tolower = FALSE)

## Transform to a matrix and view
train.tokens.matrix=as.matrix(train.tokens.dfm)
View(train.tokens.matrix[1:20,1:100])
dim(train.tokens.matrix)

## Investigating the effect of Stemming
colnames(train.tokens.matrix)[1:50]

## Setup the feature data with the labels
train.tokens.df=cbind(Label=train$Label,as.data.frame(train.tokens.dfm))

## Tokenization requires additional pre-processing
names(train.tokens.df)[c(146,148,235,238)]

## Cleanup Column names
names(train.tokens.df)=make.names(names(train.tokens.df))
names(train.tokens.df)[c(146,148,235,238)]

## Use caret for stratified cross validation repeated 3 times (ie create 30 random samples)
set.seed(48743)
cv.folds=createMultiFolds(train$Label,k=10,times = 3)
cv.cntrl=trainControl(method = "repeatedcv",number = 10,repeats = 3,index = cv.folds)

## install.package("doSNOW") This package is for speed up the cross validation
library(doSNOW)

## Time the code execution
start.time=Sys.time()
start.time

## Create a cluster to work on 3 logical cores
cl=makeCluster(3,type = "SOCK")
registerDoSNOW(cl)

## Use a single decision tree algorithm as our first model
rpart.cv.1=train(Label ~ .,data=train.tokens.df,method="rpart",trControl=cv.cntrl,tuneLength=7)

## Processing is done, stop cluster
stopCluster(cl)

## Total time of execution on workstation was approxiamtely 4 Minutes
total.time=Sys.time() - start.time
total.time

## Check out our results
rpart.cv.1

## Our function for calculating relative Term Frequency(TF)
term.frequency=function(row){row/sum(row)}

## Our function for calculating Inverse Document frequncies(IDF)
inverse.doc.freq=function(col){
  corpus.size=length(col)
  doc.count=length(which(col>0))
  log10(corpus.size/doc.count)
}

##  Our function for calculating TF-IDF
tf.idf=function(tf,idf){tf*idf}

## First step, Normalize all documents via TF
train.tokens.df=apply(train.tokens.matrix,1,term.frequency)
dim(train.tokens.df)  ## Transpose the data from horizontal to vertical
View(train.tokens.df[1:20,1:100])

## Second step, calcuating the IDF vector that we will use-both for training data & for test data
train.tokens.idf=apply(train.tokens.matrix,2,inverse.doc.freq)
str(train.tokens.idf)

##Lastly, calculate TF-IDF for our training corpus
train.tokens.tfidf=apply(train.tokens.df,2,tf.idf,idf=train.tokens.idf)
dim(train.tokens.tfidf)
View(train.tokens.tfidf[1:25,1:25])

## Transpose the Matrix
train.tokens.tfidf=t(train.tokens.tfidf)
dim(train.tokens.tfidf)
View(train.tokens.tfidf[1:25,1:25])


## Check for incomplete cases
incomplete.cases=which(!complete.cases(train.tokens.tfidf))
train$Text[incomplete.cases]

## Fix incomplete cases with zeros
train.tokens.tfidf[incomplete.cases,]=rep(0.0,ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.tfidf)))
